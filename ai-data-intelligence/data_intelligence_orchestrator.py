"""
AI Data Intelligence Platform - Revolutionary Data-Driven Agent System

This platform creates AI agents that can:
- Automatically discover and analyze data sources
- Generate intelligent insights and predictions
- Create dynamic reports and dashboards
- Provide real-time business intelligence
- Predict trends and anomalies
- Recommend data-driven actions

Revolutionary Features:
- Self-learning data models
- Autonomous data pipeline management
- Natural language data queries
- Predictive business intelligence
- Real-time anomaly detection
- Automated report generation
"""

import asyncio
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from datetime import datetime, timedelta
import json
import sqlite3
from enum import Enum

# AI and ML imports
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

try:
    from sklearn.ensemble import IsolationForest, RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, r2_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class DataSourceType(Enum):
    """Types of data sources"""
    DATABASE = "database"
    API = "api"
    FILE = "file"
    STREAM = "stream"
    CLOUD = "cloud"

class AnalysisType(Enum):
    """Types of data analysis"""
    DESCRIPTIVE = "descriptive"
    DIAGNOSTIC = "diagnostic"
    PREDICTIVE = "predictive"
    PRESCRIPTIVE = "prescriptive"

@dataclass
class DataSource:
    """Data source configuration"""
    id: str
    name: str
    type: DataSourceType
    connection_string: str
    schema: Dict[str, str]
    refresh_interval: int  # minutes
    last_updated: str
    status: str = "active"

@dataclass
class DataInsight:
    """Data insight generated by AI"""
    id: str
    title: str
    description: str
    confidence: float
    data_source: str
    analysis_type: AnalysisType
    metrics: Dict[str, Any]
    recommendations: List[str]
    created_at: str
    priority: str = "medium"

@dataclass
class Report:
    """AI-generated report"""
    id: str
    title: str
    description: str
    data_sources: List[str]
    insights: List[str]
    visualizations: List[Dict]
    created_at: str
    created_by: str
    format: str = "interactive"

class DataIntelligenceOrchestrator:
    """Central orchestrator for AI-driven data intelligence"""
    
    def __init__(self):
        self.logger = logging.getLogger("data_intelligence")
        self.data_sources: Dict[str, DataSource] = {}
        self.insights: Dict[str, DataInsight] = {}
        self.reports: Dict[str, Report] = {}
        self.data_cache: Dict[str, pd.DataFrame] = {}
        self.models: Dict[str, Any] = {}
        
        # Initialize components
        self._initialize_database()
        self._initialize_ai_models()
        self._load_default_data_sources()
    
    def _initialize_database(self):
        """Initialize the data intelligence database"""
        try:
            self.db_path = "data_intelligence.db"
            conn = sqlite3.connect(self.db_path)
            
            # Create tables
            conn.execute("""
                CREATE TABLE IF NOT EXISTS data_sources (
                    id TEXT PRIMARY KEY,
                    name TEXT,
                    type TEXT,
                    connection_string TEXT,
                    schema TEXT,
                    refresh_interval INTEGER,
                    last_updated TEXT,
                    status TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS insights (
                    id TEXT PRIMARY KEY,
                    title TEXT,
                    description TEXT,
                    confidence REAL,
                    data_source TEXT,
                    analysis_type TEXT,
                    metrics TEXT,
                    recommendations TEXT,
                    created_at TEXT,
                    priority TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS reports (
                    id TEXT PRIMARY KEY,
                    title TEXT,
                    description TEXT,
                    data_sources TEXT,
                    insights TEXT,
                    visualizations TEXT,
                    created_at TEXT,
                    created_by TEXT,
                    format TEXT
                )
            """)
            
            conn.commit()
            conn.close()
            
            self.logger.info("‚úÖ Data intelligence database initialized")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {e}")
    
    def _initialize_ai_models(self):
        """Initialize AI models for data analysis"""
        try:
            if SKLEARN_AVAILABLE:
                # Anomaly detection model
                self.models["anomaly_detector"] = IsolationForest(
                    contamination=0.1,
                    random_state=42
                )
                
                # Prediction model
                self.models["predictor"] = RandomForestRegressor(
                    n_estimators=100,
                    random_state=42
                )
                
                # Data scaler
                self.models["scaler"] = StandardScaler()
                
                self.logger.info("‚úÖ AI models initialized")
            
        except Exception as e:
            self.logger.error(f"AI model initialization failed: {e}")
    
    def _load_default_data_sources(self):
        """Load default data sources for demonstration"""
        try:
            # Application performance data
            app_performance = DataSource(
                id="app_performance",
                name="Application Performance Metrics",
                type=DataSourceType.DATABASE,
                connection_string="sqlite:///app_metrics.db",
                schema={
                    "timestamp": "datetime",
                    "response_time": "float",
                    "cpu_usage": "float",
                    "memory_usage": "float",
                    "request_count": "int",
                    "error_rate": "float"
                },
                refresh_interval=5,
                last_updated=datetime.now().isoformat()
            )
            
            # User analytics data
            user_analytics = DataSource(
                id="user_analytics",
                name="User Behavior Analytics",
                type=DataSourceType.API,
                connection_string="https://api.analytics.com/v1/users",
                schema={
                    "user_id": "string",
                    "session_duration": "float",
                    "page_views": "int",
                    "conversion_rate": "float",
                    "bounce_rate": "float",
                    "device_type": "string"
                },
                refresh_interval=15,
                last_updated=datetime.now().isoformat()
            )
            
            # Business metrics data
            business_metrics = DataSource(
                id="business_metrics",
                name="Business KPI Metrics",
                type=DataSourceType.FILE,
                connection_string="data/business_metrics.csv",
                schema={
                    "date": "datetime",
                    "revenue": "float",
                    "users": "int",
                    "conversion_rate": "float",
                    "customer_satisfaction": "float",
                    "churn_rate": "float"
                },
                refresh_interval=60,
                last_updated=datetime.now().isoformat()
            )
            
            self.data_sources = {
                "app_performance": app_performance,
                "user_analytics": user_analytics,
                "business_metrics": business_metrics
            }
            
            self.logger.info("‚úÖ Default data sources loaded")
            
        except Exception as e:
            self.logger.error(f"Failed to load default data sources: {e}")
    
    async def discover_data_sources(self, search_paths: List[str]) -> List[DataSource]:
        """AI agent automatically discovers data sources"""
        discovered_sources = []
        
        try:
            for path in search_paths:
                path_obj = Path(path)
                
                if path_obj.exists():
                    # Discover CSV files
                    for csv_file in path_obj.rglob("*.csv"):
                        try:
                            # Sample the file to understand schema
                            sample_df = pd.read_csv(csv_file, nrows=5)
                            schema = {col: str(dtype) for col, dtype in sample_df.dtypes.items()}
                            
                            data_source = DataSource(
                                id=f"csv_{csv_file.stem}",
                                name=f"CSV: {csv_file.name}",
                                type=DataSourceType.FILE,
                                connection_string=str(csv_file),
                                schema=schema,
                                refresh_interval=60,
                                last_updated=datetime.now().isoformat()
                            )
                            
                            discovered_sources.append(data_source)
                            
                        except Exception as e:
                            self.logger.warning(f"Failed to analyze {csv_file}: {e}")
                    
                    # Discover JSON files
                    for json_file in path_obj.rglob("*.json"):
                        try:
                            with open(json_file, 'r') as f:
                                sample_data = json.load(f)
                            
                            if isinstance(sample_data, list) and sample_data:
                                schema = {key: type(value).__name__ for key, value in sample_data[0].items()}
                            elif isinstance(sample_data, dict):
                                schema = {key: type(value).__name__ for key, value in sample_data.items()}
                            else:
                                continue
                            
                            data_source = DataSource(
                                id=f"json_{json_file.stem}",
                                name=f"JSON: {json_file.name}",
                                type=DataSourceType.FILE,
                                connection_string=str(json_file),
                                schema=schema,
                                refresh_interval=60,
                                last_updated=datetime.now().isoformat()
                            )
                            
                            discovered_sources.append(data_source)
                            
                        except Exception as e:
                            self.logger.warning(f"Failed to analyze {json_file}: {e}")
            
            self.logger.info(f"‚úÖ Discovered {len(discovered_sources)} data sources")
            return discovered_sources
            
        except Exception as e:
            self.logger.error(f"Data source discovery failed: {e}")
            return []
    
    async def load_data(self, source_id: str) -> Optional[pd.DataFrame]:
        """Load data from a specific source"""
        try:
            if source_id not in self.data_sources:
                self.logger.error(f"Data source {source_id} not found")
                return None
            
            source = self.data_sources[source_id]
            
            # Check cache first
            if source_id in self.data_cache:
                cache_time = datetime.fromisoformat(source.last_updated)
                if (datetime.now() - cache_time).total_seconds() < source.refresh_interval * 60:
                    return self.data_cache[source_id]
            
            # Load data based on source type
            if source.type == DataSourceType.FILE:
                if source.connection_string.endswith('.csv'):
                    df = pd.read_csv(source.connection_string)
                elif source.connection_string.endswith('.json'):
                    df = pd.read_json(source.connection_string)
                else:
                    # Generate sample data for demo
                    df = self._generate_sample_data(source_id)
            
            elif source.type == DataSourceType.DATABASE:
                # For demo, generate sample data
                df = self._generate_sample_data(source_id)
            
            elif source.type == DataSourceType.API:
                # For demo, generate sample data
                df = self._generate_sample_data(source_id)
            
            else:
                df = self._generate_sample_data(source_id)
            
            # Cache the data
            self.data_cache[source_id] = df
            
            # Update last_updated timestamp
            source.last_updated = datetime.now().isoformat()
            
            self.logger.info(f"‚úÖ Loaded data from {source_id}: {len(df)} rows")
            return df
            
        except Exception as e:
            self.logger.error(f"Failed to load data from {source_id}: {e}")
            return None
    
    def _generate_sample_data(self, source_id: str) -> pd.DataFrame:
        """Generate sample data for demonstration"""
        try:
            if source_id == "app_performance":
                # Generate performance metrics data
                dates = pd.date_range(
                    start=datetime.now() - timedelta(days=30),
                    end=datetime.now(),
                    freq='H'
                )
                
                np.random.seed(42)
                data = {
                    'timestamp': dates,
                    'response_time': np.random.normal(200, 50, len(dates)),
                    'cpu_usage': np.random.normal(60, 15, len(dates)),
                    'memory_usage': np.random.normal(70, 10, len(dates)),
                    'request_count': np.random.poisson(1000, len(dates)),
                    'error_rate': np.random.exponential(0.02, len(dates))
                }
                
                # Add some anomalies
                anomaly_indices = np.random.choice(len(dates), size=10, replace=False)
                for idx in anomaly_indices:
                    data['response_time'][idx] *= 3
                    data['error_rate'][idx] *= 5
                
                return pd.DataFrame(data)
            
            elif source_id == "user_analytics":
                # Generate user analytics data
                np.random.seed(42)
                n_users = 10000
                
                data = {
                    'user_id': [f"user_{i}" for i in range(n_users)],
                    'session_duration': np.random.exponential(300, n_users),  # seconds
                    'page_views': np.random.poisson(8, n_users),
                    'conversion_rate': np.random.beta(2, 8, n_users),
                    'bounce_rate': np.random.beta(3, 7, n_users),
                    'device_type': np.random.choice(['desktop', 'mobile', 'tablet'], n_users, p=[0.5, 0.4, 0.1])
                }
                
                return pd.DataFrame(data)
            
            elif source_id == "business_metrics":
                # Generate business metrics data
                dates = pd.date_range(
                    start=datetime.now() - timedelta(days=90),
                    end=datetime.now(),
                    freq='D'
                )
                
                np.random.seed(42)
                base_revenue = 100000
                base_users = 5000
                
                data = {
                    'date': dates,
                    'revenue': [base_revenue * (1 + 0.1 * np.sin(i/10) + np.random.normal(0, 0.05)) 
                               for i in range(len(dates))],
                    'users': [base_users * (1 + 0.05 * np.sin(i/15) + np.random.normal(0, 0.03)) 
                             for i in range(len(dates))],
                    'conversion_rate': np.random.normal(0.05, 0.01, len(dates)),
                    'customer_satisfaction': np.random.normal(4.2, 0.3, len(dates)),
                    'churn_rate': np.random.normal(0.03, 0.005, len(dates))
                }
                
                return pd.DataFrame(data)
            
            else:
                # Generic sample data
                return pd.DataFrame({
                    'timestamp': pd.date_range(start='2025-01-01', periods=100, freq='D'),
                    'value': np.random.normal(100, 20, 100),
                    'category': np.random.choice(['A', 'B', 'C'], 100)
                })
                
        except Exception as e:
            self.logger.error(f"Failed to generate sample data for {source_id}: {e}")
            return pd.DataFrame()
    
    async def analyze_data(self, source_id: str, analysis_type: AnalysisType) -> List[DataInsight]:
        """AI agent analyzes data and generates insights"""
        insights = []
        
        try:
            df = await self.load_data(source_id)
            if df is None or df.empty:
                return insights
            
            if analysis_type == AnalysisType.DESCRIPTIVE:
                insights.extend(await self._descriptive_analysis(source_id, df))
            
            elif analysis_type == AnalysisType.DIAGNOSTIC:
                insights.extend(await self._diagnostic_analysis(source_id, df))
            
            elif analysis_type == AnalysisType.PREDICTIVE:
                insights.extend(await self._predictive_analysis(source_id, df))
            
            elif analysis_type == AnalysisType.PRESCRIPTIVE:
                insights.extend(await self._prescriptive_analysis(source_id, df))
            
            # Store insights
            for insight in insights:
                self.insights[insight.id] = insight
            
            self.logger.info(f"‚úÖ Generated {len(insights)} insights for {source_id}")
            return insights
            
        except Exception as e:
            self.logger.error(f"Data analysis failed for {source_id}: {e}")
            return []
    
    async def _descriptive_analysis(self, source_id: str, df: pd.DataFrame) -> List[DataInsight]:
        """Perform descriptive analysis"""
        insights = []
        
        try:
            # Basic statistics insight
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                stats = df[numeric_cols].describe()
                
                insight = DataInsight(
                    id=f"{source_id}_descriptive_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    title="Descriptive Statistics Summary",
                    description=f"Statistical summary of {len(numeric_cols)} numeric columns in {source_id}",
                    confidence=0.95,
                    data_source=source_id,
                    analysis_type=AnalysisType.DESCRIPTIVE,
                    metrics={
                        "total_records": len(df),
                        "numeric_columns": len(numeric_cols),
                        "missing_values": df.isnull().sum().to_dict(),
                        "statistics": stats.to_dict()
                    },
                    recommendations=[
                        "Review data quality for columns with high missing values",
                        "Consider data normalization for columns with high variance",
                        "Investigate outliers in columns with extreme values"
                    ],
                    created_at=datetime.now().isoformat(),
                    priority="medium"
                )
                
                insights.append(insight)
            
            # Data quality insight
            missing_percentage = (df.isnull().sum() / len(df)) * 100
            quality_issues = missing_percentage[missing_percentage > 5].to_dict()
            
            if quality_issues:
                insight = DataInsight(
                    id=f"{source_id}_quality_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    title="Data Quality Issues Detected",
                    description=f"Found {len(quality_issues)} columns with significant missing data",
                    confidence=0.90,
                    data_source=source_id,
                    analysis_type=AnalysisType.DESCRIPTIVE,
                    metrics={
                        "quality_issues": quality_issues,
                        "overall_completeness": float((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100)
                    },
                    recommendations=[
                        "Implement data validation at the source",
                        "Consider imputation strategies for missing values",
                        "Set up monitoring alerts for data quality thresholds"
                    ],
                    created_at=datetime.now().isoformat(),
                    priority="high"
                )
                
                insights.append(insight)
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Descriptive analysis failed: {e}")
            return []
    
    async def _diagnostic_analysis(self, source_id: str, df: pd.DataFrame) -> List[DataInsight]:
        """Perform diagnostic analysis"""
        insights = []
        
        try:
            # Correlation analysis
            numeric_df = df.select_dtypes(include=[np.number])
            if len(numeric_df.columns) > 1:
                correlation_matrix = numeric_df.corr()
                
                # Find high correlations
                high_correlations = []
                for i in range(len(correlation_matrix.columns)):
                    for j in range(i+1, len(correlation_matrix.columns)):
                        corr_value = correlation_matrix.iloc[i, j]
                        if abs(corr_value) > 0.7:
                            high_correlations.append({
                                'feature1': correlation_matrix.columns[i],
                                'feature2': correlation_matrix.columns[j],
                                'correlation': float(corr_value)
                            })
                
                if high_correlations:
                    insight = DataInsight(
                        id=f"{source_id}_correlation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        title="Strong Correlations Detected",
                        description=f"Found {len(high_correlations)} pairs of highly correlated features",
                        confidence=0.85,
                        data_source=source_id,
                        analysis_type=AnalysisType.DIAGNOSTIC,
                        metrics={
                            "high_correlations": high_correlations,
                            "correlation_matrix": correlation_matrix.to_dict()
                        },
                        recommendations=[
                            "Consider feature selection to reduce multicollinearity",
                            "Investigate causal relationships between correlated features",
                            "Use correlation insights for feature engineering"
                        ],
                        created_at=datetime.now().isoformat(),
                        priority="medium"
                    )
                    
                    insights.append(insight)
            
            # Anomaly detection
            if SKLEARN_AVAILABLE and len(numeric_df.columns) > 0:
                try:
                    # Prepare data for anomaly detection
                    X = numeric_df.dropna()
                    if len(X) > 10:
                        scaler = StandardScaler()
                        X_scaled = scaler.fit_transform(X)
                        
                        # Detect anomalies
                        isolation_forest = IsolationForest(contamination=0.1, random_state=42)
                        anomaly_labels = isolation_forest.fit_predict(X_scaled)
                        
                        # Count anomalies
                        anomaly_count = np.sum(anomaly_labels == -1)
                        anomaly_percentage = (anomaly_count / len(X)) * 100
                        
                        if anomaly_count > 0:
                            insight = DataInsight(
                                id=f"{source_id}_anomalies_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                title="Anomalies Detected in Data",
                                description=f"Detected {anomaly_count} anomalous records ({anomaly_percentage:.1f}%)",
                                confidence=0.75,
                                data_source=source_id,
                                analysis_type=AnalysisType.DIAGNOSTIC,
                                metrics={
                                    "anomaly_count": int(anomaly_count),
                                    "anomaly_percentage": float(anomaly_percentage),
                                    "total_records_analyzed": len(X)
                                },
                                recommendations=[
                                    "Investigate anomalous records for data quality issues",
                                    "Set up real-time anomaly detection monitoring",
                                    "Consider excluding anomalies from predictive models"
                                ],
                                created_at=datetime.now().isoformat(),
                                priority="high"
                            )
                            
                            insights.append(insight)
                
                except Exception as e:
                    self.logger.warning(f"Anomaly detection failed: {e}")
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Diagnostic analysis failed: {e}")
            return []
    
    async def _predictive_analysis(self, source_id: str, df: pd.DataFrame) -> List[DataInsight]:
        """Perform predictive analysis"""
        insights = []
        
        try:
            numeric_df = df.select_dtypes(include=[np.number])
            
            if len(numeric_df.columns) < 2:
                return insights
            
            # Time series prediction if timestamp column exists
            if 'timestamp' in df.columns or 'date' in df.columns:
                time_col = 'timestamp' if 'timestamp' in df.columns else 'date'
                
                try:
                    df_time = df.copy()
                    df_time[time_col] = pd.to_datetime(df_time[time_col])
                    df_time = df_time.sort_values(time_col)
                    
                    # Simple trend analysis
                    for col in numeric_df.columns:
                        if col != time_col:
                            values = df_time[col].dropna()
                            if len(values) > 10:
                                # Calculate trend
                                x = np.arange(len(values))
                                trend = np.polyfit(x, values, 1)[0]
                                
                                # Predict next 7 days
                                next_week = len(values) + 7
                                predicted_value = np.polyval([trend, values.iloc[-1]], 7)
                                
                                insight = DataInsight(
                                    id=f"{source_id}_trend_{col}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                    title=f"Trend Analysis for {col}",
                                    description=f"Detected {'increasing' if trend > 0 else 'decreasing'} trend in {col}",
                                    confidence=0.70,
                                    data_source=source_id,
                                    analysis_type=AnalysisType.PREDICTIVE,
                                    metrics={
                                        "trend_slope": float(trend),
                                        "current_value": float(values.iloc[-1]),
                                        "predicted_value_7d": float(predicted_value),
                                        "trend_direction": "increasing" if trend > 0 else "decreasing"
                                    },
                                    recommendations=[
                                        f"Monitor {col} for continued {'growth' if trend > 0 else 'decline'}",
                                        "Consider implementing trend-based alerts",
                                        "Use trend information for capacity planning"
                                    ],
                                    created_at=datetime.now().isoformat(),
                                    priority="medium"
                                )
                                
                                insights.append(insight)
                
                except Exception as e:
                    self.logger.warning(f"Time series analysis failed: {e}")
            
            # Feature importance analysis
            if SKLEARN_AVAILABLE and len(numeric_df.columns) > 2:
                try:
                    # Use the first numeric column as target
                    target_col = numeric_df.columns[0]
                    feature_cols = numeric_df.columns[1:]
                    
                    # Prepare data
                    clean_df = numeric_df.dropna()
                    if len(clean_df) > 20:
                        X = clean_df[feature_cols]
                        y = clean_df[target_col]
                        
                        # Train a simple model
                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                        
                        rf_model = RandomForestRegressor(n_estimators=50, random_state=42)
                        rf_model.fit(X_train, y_train)
                        
                        # Get feature importance
                        importance = rf_model.feature_importances_
                        feature_importance = dict(zip(feature_cols, importance))
                        
                        # Model performance
                        y_pred = rf_model.predict(X_test)
                        r2 = r2_score(y_test, y_pred)
                        
                        insight = DataInsight(
                            id=f"{source_id}_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            title=f"Predictive Model for {target_col}",
                            description=f"Built predictive model with R¬≤ score of {r2:.3f}",
                            confidence=min(0.95, max(0.5, r2)),
                            data_source=source_id,
                            analysis_type=AnalysisType.PREDICTIVE,
                            metrics={
                                "model_r2_score": float(r2),
                                "feature_importance": {k: float(v) for k, v in feature_importance.items()},
                                "target_variable": target_col,
                                "training_samples": len(X_train)
                            },
                            recommendations=[
                                f"Focus on top features: {', '.join(sorted(feature_importance.keys(), key=feature_importance.get, reverse=True)[:3])}",
                                "Consider collecting more data for features with low importance",
                                "Implement model-based predictions for business planning"
                            ],
                            created_at=datetime.now().isoformat(),
                            priority="high"
                        )
                        
                        insights.append(insight)
                
                except Exception as e:
                    self.logger.warning(f"Predictive modeling failed: {e}")
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Predictive analysis failed: {e}")
            return []
    
    async def _prescriptive_analysis(self, source_id: str, df: pd.DataFrame) -> List[DataInsight]:
        """Perform prescriptive analysis"""
        insights = []
        
        try:
            # Business optimization recommendations
            numeric_df = df.select_dtypes(include=[np.number])
            
            if source_id == "app_performance":
                # Performance optimization recommendations
                if 'response_time' in df.columns and 'cpu_usage' in df.columns:
                    avg_response_time = df['response_time'].mean()
                    max_cpu_usage = df['cpu_usage'].max()
                    
                    recommendations = []
                    if avg_response_time > 500:
                        recommendations.append("Implement caching to reduce response times")
                        recommendations.append("Optimize database queries")
                        recommendations.append("Consider CDN for static assets")
                    
                    if max_cpu_usage > 80:
                        recommendations.append("Scale horizontally to distribute CPU load")
                        recommendations.append("Optimize CPU-intensive algorithms")
                        recommendations.append("Implement auto-scaling policies")
                    
                    if recommendations:
                        insight = DataInsight(
                            id=f"{source_id}_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            title="Performance Optimization Recommendations",
                            description="AI-generated recommendations to improve application performance",
                            confidence=0.85,
                            data_source=source_id,
                            analysis_type=AnalysisType.PRESCRIPTIVE,
                            metrics={
                                "current_avg_response_time": float(avg_response_time),
                                "max_cpu_usage": float(max_cpu_usage),
                                "target_response_time": 200.0,
                                "target_cpu_usage": 70.0
                            },
                            recommendations=recommendations,
                            created_at=datetime.now().isoformat(),
                            priority="high"
                        )
                        
                        insights.append(insight)
            
            elif source_id == "business_metrics":
                # Business optimization recommendations
                if 'revenue' in df.columns and 'conversion_rate' in df.columns:
                    avg_conversion = df['conversion_rate'].mean()
                    revenue_trend = df['revenue'].pct_change().mean()
                    
                    recommendations = []
                    if avg_conversion < 0.05:
                        recommendations.append("Implement A/B testing for conversion optimization")
                        recommendations.append("Improve user onboarding experience")
                        recommendations.append("Optimize call-to-action placements")
                    
                    if revenue_trend < 0.01:
                        recommendations.append("Develop new customer acquisition channels")
                        recommendations.append("Implement customer retention programs")
                        recommendations.append("Consider premium feature offerings")
                    
                    if recommendations:
                        insight = DataInsight(
                            id=f"{source_id}_business_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            title="Business Growth Optimization",
                            description="Strategic recommendations to improve business metrics",
                            confidence=0.80,
                            data_source=source_id,
                            analysis_type=AnalysisType.PRESCRIPTIVE,
                            metrics={
                                "current_conversion_rate": float(avg_conversion),
                                "revenue_growth_rate": float(revenue_trend),
                                "target_conversion_rate": 0.08,
                                "target_growth_rate": 0.05
                            },
                            recommendations=recommendations,
                            created_at=datetime.now().isoformat(),
                            priority="high"
                        )
                        
                        insights.append(insight)
            
            # General optimization recommendations
            if len(numeric_df.columns) > 0:
                # Identify optimization opportunities
                optimization_opportunities = []
                
                for col in numeric_df.columns:
                    values = df[col].dropna()
                    if len(values) > 0:
                        cv = values.std() / values.mean() if values.mean() != 0 else 0
                        if cv > 0.5:  # High variability
                            optimization_opportunities.append(f"Reduce variability in {col}")
                
                if optimization_opportunities:
                    insight = DataInsight(
                        id=f"{source_id}_general_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        title="General Optimization Opportunities",
                        description=f"Identified {len(optimization_opportunities)} areas for optimization",
                        confidence=0.70,
                        data_source=source_id,
                        analysis_type=AnalysisType.PRESCRIPTIVE,
                        metrics={
                            "optimization_count": len(optimization_opportunities),
                            "high_variability_metrics": optimization_opportunities
                        },
                        recommendations=optimization_opportunities + [
                            "Implement process standardization",
                            "Set up monitoring for key metrics",
                            "Create performance benchmarks"
                        ],
                        created_at=datetime.now().isoformat(),
                        priority="medium"
                    )
                    
                    insights.append(insight)
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Prescriptive analysis failed: {e}")
            return []
    
    async def generate_report(self, title: str, data_sources: List[str], 
                            analysis_types: List[AnalysisType]) -> Report:
        """Generate comprehensive AI-driven report"""
        try:
            report_id = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Collect insights from all sources
            all_insights = []
            visualizations = []
            
            for source_id in data_sources:
                for analysis_type in analysis_types:
                    insights = await self.analyze_data(source_id, analysis_type)
                    all_insights.extend(insights)
                
                # Generate visualizations
                viz = await self._generate_visualizations(source_id)
                if viz:
                    visualizations.extend(viz)
            
            # Create report
            report = Report(
                id=report_id,
                title=title,
                description=f"AI-generated report analyzing {len(data_sources)} data sources with {len(all_insights)} insights",
                data_sources=data_sources,
                insights=[insight.id for insight in all_insights],
                visualizations=visualizations,
                created_at=datetime.now().isoformat(),
                created_by="AI Data Intelligence Agent"
            )
            
            # Store report
            self.reports[report_id] = report
            
            self.logger.info(f"‚úÖ Generated report {report_id} with {len(all_insights)} insights")
            return report
            
        except Exception as e:
            self.logger.error(f"Report generation failed: {e}")
            return None
    
    async def _generate_visualizations(self, source_id: str) -> List[Dict]:
        """Generate visualizations for data source"""
        visualizations = []
        
        try:
            if not PLOTLY_AVAILABLE:
                return visualizations
            
            df = await self.load_data(source_id)
            if df is None or df.empty:
                return visualizations
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            # Time series chart if timestamp exists
            if ('timestamp' in df.columns or 'date' in df.columns) and len(numeric_cols) > 0:
                time_col = 'timestamp' if 'timestamp' in df.columns else 'date'
                
                for col in numeric_cols[:3]:  # Limit to first 3 numeric columns
                    fig = px.line(df, x=time_col, y=col, title=f"{col} Over Time")
                    visualizations.append({
                        'id': f"{source_id}_{col}_timeseries",
                        'title': f"{col} Time Series",
                        'type': 'line_chart',
                        'config': fig.to_dict(),
                        'data_source': source_id
                    })
            
            # Distribution charts
            for col in numeric_cols[:2]:  # Limit to first 2 numeric columns
                fig = px.histogram(df, x=col, title=f"Distribution of {col}")
                visualizations.append({
                    'id': f"{source_id}_{col}_distribution",
                    'title': f"{col} Distribution",
                    'type': 'histogram',
                    'config': fig.to_dict(),
                    'data_source': source_id
                })
            
            # Correlation heatmap if multiple numeric columns
            if len(numeric_cols) > 1:
                correlation_matrix = df[numeric_cols].corr()
                fig = px.imshow(correlation_matrix, 
                               title="Feature Correlations",
                               color_continuous_scale="RdBu",
                               aspect="auto")
                visualizations.append({
                    'id': f"{source_id}_correlation_heatmap",
                    'title': "Correlation Matrix",
                    'type': 'heatmap',
                    'config': fig.to_dict(),
                    'data_source': source_id
                })
            
            return visualizations
            
        except Exception as e:
            self.logger.error(f"Visualization generation failed for {source_id}: {e}")
            return []
    
    def get_data_sources(self) -> Dict[str, DataSource]:
        """Get all available data sources"""
        return self.data_sources
    
    def get_insights(self, source_id: Optional[str] = None) -> List[DataInsight]:
        """Get insights, optionally filtered by source"""
        if source_id:
            return [insight for insight in self.insights.values() 
                   if insight.data_source == source_id]
        return list(self.insights.values())
    
    def get_reports(self) -> List[Report]:
        """Get all generated reports"""
        return list(self.reports.values())
    
    async def get_dashboard_data(self) -> Dict[str, Any]:
        """Get comprehensive dashboard data"""
        try:
            dashboard_data = {
                "summary": {
                    "total_data_sources": len(self.data_sources),
                    "total_insights": len(self.insights),
                    "total_reports": len(self.reports),
                    "last_updated": datetime.now().isoformat()
                },
                "data_sources": [asdict(source) for source in self.data_sources.values()],
                "recent_insights": [asdict(insight) for insight in 
                                  sorted(self.insights.values(), 
                                        key=lambda x: x.created_at, reverse=True)[:10]],
                "reports": [asdict(report) for report in self.reports.values()]
            }
            
            return dashboard_data
            
        except Exception as e:
            self.logger.error(f"Failed to get dashboard data: {e}")
            return {}

# Global instance
data_intelligence = DataIntelligenceOrchestrator()

# Example usage
async def demo_data_intelligence():
    """Demonstrate data intelligence capabilities"""
    
    print("üìä AI Data Intelligence Platform Demo")
    print("=" * 50)
    
    # Show data sources
    sources = data_intelligence.get_data_sources()
    print(f"\nüìÅ Available Data Sources: {len(sources)}")
    for source_id, source in sources.items():
        print(f"   ‚Ä¢ {source.name} ({source.type.value})")
    
    # Analyze data
    print(f"\nüîç Running AI Analysis...")
    
    for source_id in ["app_performance", "business_metrics"]:
        print(f"\nAnalyzing {source_id}:")
        
        # Run all types of analysis
        for analysis_type in AnalysisType:
            insights = await data_intelligence.analyze_data(source_id, analysis_type)
            print(f"   {analysis_type.value}: {len(insights)} insights generated")
    
    # Generate comprehensive report
    print(f"\nüìã Generating Comprehensive Report...")
    report = await data_intelligence.generate_report(
        title="AI-Driven System Analysis Report",
        data_sources=["app_performance", "business_metrics"],
        analysis_types=[AnalysisType.DESCRIPTIVE, AnalysisType.PREDICTIVE, AnalysisType.PRESCRIPTIVE]
    )
    
    if report:
        print(f"‚úÖ Report generated: {report.title}")
        print(f"   üìä {len(report.insights)} insights included")
        print(f"   üìà {len(report.visualizations)} visualizations created")
    
    # Show dashboard summary
    dashboard_data = await data_intelligence.get_dashboard_data()
    print(f"\nüìä Dashboard Summary:")
    print(f"   Data Sources: {dashboard_data['summary']['total_data_sources']}")
    print(f"   Total Insights: {dashboard_data['summary']['total_insights']}")
    print(f"   Generated Reports: {dashboard_data['summary']['total_reports']}")
    
    print(f"\nüéâ AI Data Intelligence Platform is operational!")
    print("Ready to transform your data into actionable insights! üìà")

if __name__ == "__main__":
    asyncio.run(demo_data_intelligence())
